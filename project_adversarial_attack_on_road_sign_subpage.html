<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Adversarial Attacks on Road Sign Recognition — Aditya Kothari</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=Playfair+Display:wght@600;700&display=swap" rel="stylesheet" />
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet" />
  <style>
    body { font-family: Inter, ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial; color:#0b1324; background:#f4f6fb; letter-spacing:.01em; }
    .h-display{ font-family: "Playfair Display", ui-serif, Georgia, Times, serif; letter-spacing:-.01em; }
    .container-w{ max-width:1120px; }
    .card{ background:#fff; border-radius:16px; padding:1.5rem; box-shadow:0 10px 25px rgba(0,0,0,.08); }
    .btn{ display:inline-flex; align-items:center; justify-content:center; border-radius:12px; padding:.75rem 1.25rem; font-weight:600; }
    .btn-primary{ background:#2563eb; color:#fff; }
    .btn-primary:hover{ background:#1d4ed8; }
    .link-underline{ position:relative; text-decoration:none; }
    .link-underline:after{ content:""; position:absolute; left:0; bottom:-2px; height:2px; width:100%; background: currentColor; opacity:.2; transform: scaleX(0); transform-origin:left; transition: transform .25s ease; }
    .link-underline:hover:after{ transform: scaleX(1); }
    .hero-img{ border-radius:16px; box-shadow:0 10px 25px rgba(0,0,0,.08); }
  </style>
</head>
<body>
  <!-- NAV -->
  <header class="bg-white border-b sticky top-0 z-50">
    <div class="container-w mx-auto px-6 py-4 flex items-center justify-between">
      <a href="/" class="flex items-center space-x-3">
        <div class="w-2.5 h-2.5 rounded-full bg-blue-600"></div>
        <span class="font-semibold tracking-wide">Aditya Kothari</span>
      </a>
      <nav class="hidden md:flex items-center space-x-6 text-sm">
        <a class="link-underline" href="/index.html#case-studies">Case Studies</a>
        <a class="link-underline" href="/index.html#projects">Projects</a>
        <a class="link-underline" href="/index.html#contact">Contact</a>
      </nav>
    </div>
  </header>

  <!-- HERO / TITLE -->
  <section class="bg-white">
    <div class="container-w mx-auto px-6 py-12 md:py-16">
      <div class="grid lg:grid-cols-12 gap-10">
        <div class="lg:col-span-8">
          <p class="text-sm tracking-widest font-semibold text-blue-700 uppercase mb-3">Project</p>
          <h1 class="h-display text-4xl md:text-5xl font-semibold leading-tight">Adversarial Attacks on Road Sign Recognition</h1>
          <p class="mt-4 text-gray-600 text-base md:text-lg max-w-2xl">Reproducing a physical patch attack (RP2) against CNN/ResNet models on GTSRB/LISA datasets. Evaluating the defenses provided via augmentation and adversarial training.</p>
          <div class="mt-6 flex flex-wrap gap-3">
            <a class="btn btn-primary" href="./resources/project_adversarial_attack_on_road_sign_recognition_report.pdf" target="_blank" rel="noopener">⬇ Read Full Report (PDF)</a>
            <a class="btn" style="background:#eef2ff;color:#1d4ed8" href="index.html#projects">← Back to Projects</a>
          </div>
        </div>
        <div class="lg:col-span-4">
          <!-- Optional key visual sourced from the report (export and place at path below) -->
          <img class="hero-img w-full" src="./images/adversarial-patch-example.png" alt="Adversarial patch applied to traffic sign example" />
          <p class="text-xs text-gray-500 mt-2">Figure: Traffic sign under attack, Generated label for attack & Traffic sign with physical
perturbation sticker.</p>
          <img class="hero-img w-full" src="./images/rp2-attack-pipeline.png"/>
          <p class="text-xs text-gray-500 mt-2">Figure: RP2 Attack Pipeline.</p>
        </div>
      </div>
    </div>
  </section>

  <!-- ABSTRACT -->
  <section class="py-16" style="background:#f6f8fb">
    <div class="container-w mx-auto px-6">
      <div class="grid lg:grid-cols-12 gap-10">
        <div class="lg:col-span-4"><h2 class="h-display text-3xl font-semibold">Abstract</h2></div>
        <div class="lg:col-span-8">
          <div class="card">
            <p class="text-gray-700 leading-relaxed">
              We study robustness of traffic sign recognition models to physical <em>patch</em> attacks, where a small printed pattern
              is affixed to a sign to induce misclassification in the wild. Using GTSRB and LISA datasets, we implement an RP2-style
              white-box attack targeting CNN/ResNet-18 models, measure attack success rate (ASR) under varying conditions, and
              evaluate defenses including input-space augmentation and adversarial training. Results highlight significant vulnerability
              of unprotected models and the partial mitigation offered by robust training protocols.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- METHOD -->
  <section class="py-16">
    <div class="container-w mx-auto px-6">
      <div class="grid lg:grid-cols-12 gap-10 items-start">
        <div class="lg:col-span-4"><h2 class="h-display text-3xl font-semibold">Adversarial Attack Method</h2></div>
        <div class="lg:col-span-8">
          <div class="card space-y-6">
            <div>
              <h3 class="font-semibold text-lg">Threat Model & Objective</h3>
              <p class="text-gray-700 mt-2">White-box access during patch optimization; physical-world patch application at inference. Objective: targeted misclassification to a chosen label while constraining patch area and printable color range.</p>
            </div>
            <div>
              <h3 class="font-semibold text-lg">Patch Optimization (RP2)</h3>
              <ul class="list-disc list-inside text-gray-700 mt-2 space-y-1">
                <li>Optimize a printable patch pattern using gradient-based updates on training images.</li>
                <li>Apply expectation over transforms (EOT): random scale, rotation, translation, brightness to emulate physical conditions.</li>
                <li>Constrain with L<sub>p</sub> and total area masks; composite onto sign region during training iterations.</li>
              </ul>
            </div>
            <div>
              <h3 class="font-semibold text-lg">Models & Datasets</h3>
              <p class="text-gray-700 mt-2">Baseline CNN and ResNet‑18 classifiers trained on GTSRB; cross-checked on LISA for transferability. Standard train/val/test splits with normalization and resizing to model input.</p>
            </div>
            <div>
              <h3 class="font-semibold text-lg">Defenses Evaluated</h3>
              <ul class="list-disc list-inside text-gray-700 mt-2 space-y-1">
                <li><strong>Augmentation:</strong> heavy geometric/photometric jitter to reduce overfitting to patch artifacts.</li>
                <li><strong>Adversarial Training:</strong> mix of clean and patched samples per batch to improve robustness.</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- RESULTS -->
  <section class="py-16" style="background:#f6f8fb">
    <div class="container-w mx-auto px-6">
      <div class="grid lg:grid-cols-12 gap-10 items-start">
        <div class="lg:col-span-4"><h2 class="h-display text-3xl font-semibold">Results</h2></div>
        <div class="lg:col-span-8">
          <div class="card space-y-6">
            <div>
              <h3 class="font-semibold text-lg">Attack Success & Accuracy Impact</h3>
              <p class="text-gray-700 mt-2">On unprotected models, targeted patches achieve high attack success rate (ASR) with a notable drop in top‑1 accuracy on patched inputs. Robust training reduces ASR and narrows the clean‑robust gap.</p>
            </div>
            <div>
              <h3 class="font-semibold text-lg">Qualitative Examples</h3>
              <figure class="mt-3">
                <img class="w-full hero-img" src="./images/adversarial-attack-success-rate.png" alt="Grid of qualitative results showing clean vs patched predictions" />
                <figcaption class="text-xs text-gray-500 mt-2">Qualitative: Performance comparison of RP2 method
against undefended models and defended models.</figcaption>
              </figure>
            </div>
            <div>
              <h3 class="font-semibold text-lg">Takeaways</h3>
              <ul class="list-disc list-inside text-gray-700 mt-2 space-y-1">
                <li>Physical patch attacks remain effective without robust training.</li>
                <li>Data augmentation helps, but adversarial training provides stronger improvements.</li>
                <li>Future work: certified defenses, improved detection of out‑of‑distribution textures, and robustness under motion blur and distance.</li>
              </ul>
            </div>
            <div class="pt-2">
              <!-- <a class="btn btn-primary" href="../projects/annotated-Adversial Attack for Road Signs Recognition - Report.pdf" target="_blank" rel="noopener">Open Full PDF</a> -->
              <a class="btn" style="background:#eef2ff;color:#1d4ed8" href="/index.html#projects">Back to Projects</a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- FOOTER -->
  <footer class="bg-white border-t">
    <div class="container-w mx-auto px-6 py-8 text-sm text-gray-500 flex items-center justify-between">
      <span>© 2025 Aditya Kothari</span>
      <a class="text-blue-700 link-underline" href="/index.html">Return to Home</a>
    </div>
  </footer>
</body>
</html>
